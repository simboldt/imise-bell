%*****************************************
\chapter{Grundlagen}\label{ch:preliminaries}
%*****************************************

\section{Medizinische Informatik}
\subsection{Informationssysteme im Gesundheitswesen}

\section{Transkription}

Die Methode der Transkription beschreibt \enquote{Die Verschriftlichung menschlicher Kommunikation, meist auf der Grundlage von Tonband- oder anderen Aufzeichnungen} \citep{transkription}.



%Im Kapitel \enquote{Literaturverzeichnis} dieser Vorlage wird beschrieben, wie eine Quellenangabe zu erfolgen hat.



\section{Question Answering}

\section{Automatische Spracherkennung (ASR)}

\section{Neuronale Netze}
Neuronale Netze basieren auf der Funktionsweise biologischer Nervenzellen und werden verwendet um verschiedene komplexe Aufgaben zu lösen.
Sie bestehen aus vielen künstlichen Neuronen in verschiedenen Schichten.
Es gibt eine Eingabeschicht, eine Ausgabeschicht und dazwischen beliebig viele versteckte Schichten.
Zwischen den Neuronen gibt es gewichtete Verbindungen (Abb. noch einfügen).
Den Neuronen der Eingabeschicht wird durch die Eingabe ein Wert zugewiesen.
Die Werte der weiteren Neuronen werden aus denen der vorherigen\input{FrontBackmatter/Declaration} Schicht berechnet.
Dafür werden diese jeweils mit dem Gewicht der Verbindung zwischen den Neuronen multipliziert und dann addiert.
Es kann auch ein Bias (beliebiger fester Wert) addiert werden.
Das Ergebnis dieser Rechnung wird durch eine weitere Funktion bearbeitet.
Die Werte der Neuronen in der Ausgabeschicht bilden die Ausgabe des neuronalen Netzes.

\subsection{Training von neuronalen Netzen, Maschinelles Lernen}
Die Gewichte der Verbindungen und der Bias bilden die Parameter des neuronalen Netzes und bestimmen dessen Funktion.
Beim Training werden Datenpaare aus Eingabe und korrekter dazugehöriger Ausgabe oder \textit{label} verwendet.
Die Ausgaben des neuronalen Netzes werden dann mit dem Label verglichen.
Aus deren Differenz wird ein Fehlerwert berechnet.
Die Parameter werden daraufhin so angepasst, dass sich dieser Fehlerwert verringert.
Wenn dieser Vorgang mit großen Mengen von Daten immer wieder durchgeführt wird, werden die Ausgaben auch für unbekannte Daten immer genauer.
Diese Form des Training nennt man überwachtes Lernen.


Eine weitere Trainingsmethoden ist das unüberwachte Lernen.
Dafür werden unmarkierte Daten verwendet.
Dem neuronalen Netz wird hier also keine Lösung vorgegeben und es soll stattdessen selbständig Muster in den Daten erkennen und diese in \textit{cluster} einteilen.


Beim Semi-überwachten lernen werden gemischte Trainingsdaten bestehend aus einem kleinen Teil gelabelter und einem großen Teil ungelabelter Daten.
Die gelabelten Daten geben dem neuronalen Netz die cluster vor und der Rest verläuft analog zum unüberwachten Lernen.


Selbstüberwachtes Lernen verläuft analog zum überwachten Lernen, jedoch unterscheidet sich die Art der Trainingsdaten.
Während diesen beim überwachten Lernen zunächst ein Label hinzugefügt werden muss, benutzt man hier einen Teil der 


\section{Sprachmodelle}

\section{Quantisierung}
Quantisierung im Kontext von Sprachmodellen ist ein Prozess, bei dem die Genauigkeit der Gewichte verringert wird.
Beispielsweise werden 32 Bit Fließkommazahlen auf einen 8-Bit-Bereich reduziert und gerundet.
Dadurch nimmt Präzision des Sprachmodells ab, allerdings wird weniger Rechenleistung benötigt und die Berechnungen laufen schneller.

\section{Transformers}
\section{Bewertung von Transkriptionsmodellen}
Die gängigste Methode zur Bewertung der Qualität von Transkriptionsmodellen ist die \ac{wer}~\citep{wer}.
Diese gibt an, wie viele Prozent der Wörter N verglichen mit einem korrekten Transkript fehlerhaft sind.
Fehler sind dabei zusätzliche Wörter I (\textit{insertion}), fehlende Wörter D (\textit{deletion}) und ausgetauschte Wörter S (\textit{substitution}):
\[\textnormal{WER} = \frac{I + D + S}{N} \times 100\]
Die \ac{wer} ist somit ein Maß für die Ungenauigkeit einer Transkription, wobei eine niedrigere \ac{wer} eine hohe Genauigkeit angibt.
Um sie möglichst genau zu ermitteln, müssen lange Audioeingaben verwendet werden.
Die Performance eines Transkriptionsmodells ist stark abhängig von der verwendeten Audiodatei und den verwendeten Geräten, weshalb sich \acp{wer} aus verschiedenen Tests oft nicht vergleichen lassen.

\section{Pseudonymisierung und Anonymisierung}
Pseudonymisierung und Anonymisierung werden genutzt, wenn personenbezogene Informationen niemandem zugeordnet werden sollen.
Es sind Wege solche Daten von Personen zu trennen um, diese nicht zu belasten.

\begin{definition}[Pseudonymisierung]
Bei der Pseudonymisierung wird der Name einer Person durch einen anderen Namen ersetzt.
%Bei Pseudonymisierung werden Name von Personen durch andere Namen ersetzt.
%Diese Namen werden dann ganz normal Verwendet.
%Pseudonymisierung ist sinnvoll, wenn man zwischen Personen unterscheiden können muss ohne diese mit ihrem echten Namen zu nennen.
\end{definition}

\begin{definition}[Anonymisierung]
Bei der Anonymisierung wird ein Name entfernt und stattdessen eine Bezeichnung wie \enquote{Schüler 1} verwendet, die man der Person nicht zuordnen kann.
%Bei Anonymisierung werden Namen von Personen entfernt. Bspw. wird dann "Max Mustermann" zu "Schüler"
\end{definition}