%*****************************************
\chapter{Stand der Forschung}\label{ch:relatedWork}
%*****************************************

In diesem Kapitel wird die verwendete Software beschrieben und die Vorarbeit erläutert, auf die sich diese Arbeit stützt.

\section{Whisper}
Whisper ist ein Open-Source-Transkriptionsmodell von OpenAI, welches mit 680.000 Stunden an Audio aus dem Internet in verschiedenen Sprachen trainiert wurde.
Aufgrund der weitreichenden Diversität der Trainingsdaten erreicht Whisper auch bei Akzenten und Fachsprache  hohe Genauigkeit.
Durch den großen Umfang des Trainings erreicht Whisper eine gute Zero-Shot-Performance, kommt also gut mit Daten zurecht, auf die es nicht spezifisch trainiert wurde.
\citep{whisper}
%Vergleich vieler ASR Modelle anhand deutsch- und englischsprachiger Youtubevideo Ausschnitte: OpenAI Whisper in allen Kategorien niedrigste \ac{wer}, vor allem in deutscher Sprache mit 5.0 bei einem Durchschnitt von 15.3 (Platz 2: Speechmatics 8.0, Platz 3: Microsoft 10.1), allerdings am langsamsten \citet{VergleichASR2023}

\subsection{Whisper-Modelle}
Es existieren verschiedene Whisper-Modelle (\cref{tab:whisper_modelle})
Das größte dieser Modelle, Whisper large, gibt es in drei Versionen:
Whisper large, Whisper large-v2, Whisper large-v3.
Whisper turbo ist eine quantisierte Version von Whisper large-v3.
Die Modelle werden mit zunehmender Größe langsamer, aber auch genauer, das heißt die \ac{wer} nimmt ab.
Whisper Turbo ist dabei eine Ausnahme, weil es ähnlich genau wie Whisper large-v2 und gleichzeitig fast so schnell wie Whisper tiny ist \citep{distilwhisper}.


\begin{table}[h]
\centering
\begin{tabulary}{\textwidth}{lS}
\toprule
\textbf{Modell} & \textbf{Parameter}\\
Tiny & \qty{39}{\mega\nounit}\\
Base & \qty{74}{\mega\nounit}\\
Small & \qty{244}{\mega\nounit}\\
Medium & \qty{769}{\mega\nounit}\\
Large & \qty{1550}{\mega\nounit}\\
Turbo & \qty{809}{\mega\nounit}\\
\bottomrule
\end{tabulary}
\caption{Whisper-Modelle}
\label{tab:whisper_modelle}
\end{table}

\subsection{Whisper im Vergleich}
In \cite{VergleichASR2023} werden verschiedene Transkriptionsmodelle, darunter Whisper large-v2, verglichen.
Die verwendeten Datensätze bestehen aus deutsch- und englischsprachigen Ausschnitten von Vorlesungen auf YouTube.
Whisper erreichte bei allen getesteten Datensätzen die niedrigste \ac{wer}, benötigte für die Transkription allerdings die meiste Zeit.


\section{Nvidia CUDA}
\ac{cuda} ist ein Programmiermodell von Nvidia, welches es ermöglicht, Nvidia-Grafikkarten für allgemeine Berechnungen zu nutzen.
Grafikprozessoren bestehen aus sehr vielen Rechenkernen und können somit sehr viele Berechnungen gleichzeitig durchführen.
Sie eignen sich dadurch gut für stark parallelisierbare Aufgaben, also Berechnungen, die sich in viele Teilaufgaben aufteilen lassen.
Dazu gehören, neben klassischen Grafikberechnungen, auch physikalische Simulationen oder die Ausführung neuronaler Netze.
\ac{cuda} macht die Rechenleistung von Grafikkarten für solche Anwendungen nutzbar.
Im Vergleich zur Ausführung über einen normalen Prozessor werden die Anwendungen dadurch erheblich beschleunigt. \citep{cuda}


\section{noScribe}
Die Software noScribe ist eine Open Source Anwendung zum Transkribieren von Audioaufnahme, welche eine Benutzeroberfläche inklusive Editor bereitstellt.
Sie ist mit den gängigen Audio- oder Videodateitypen kompatibel.
Die Software verwendet zum transkribieren standardmäßig OpenAI Whisper, ermöglicht aber optional auch das Hinzufügen anderer Modelle. 
NoScribe läuft vollständig lokal, verwendet also keine externen Server für die Berechnung.
Als Ergebnis der Transkription erhält man eine HTML-Datei, welche während des Vorgangs regelmäßig automatisch gespeichert wird.
Das ist hilfreich, falls das Programm zwischendurch abstürzt oder auf anderem Weg unterbrochen wird.
Neben der Standardversion von noScribe gibt es eine weitere Variante, welche Nvidia CUDA verwendet, um die Prozesse über die Grafikkarte auszuführen und damit stark zu beschleunigen.
Dafür wird eine Nvidia-Grafikkarte mit mindestens 6 GB VRAM benötigt.
\citep{noscribe}

